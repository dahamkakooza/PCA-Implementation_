# Performance benchmarking
# TO DO: Optimize implementation and benchmark performance

print("Performance Benchmarking:")
print("=" * 50)

# Benchmark with different dataset sizes
dataset_sizes = [100, 500, 1000, 5000]
n_features = X_scaled.shape[1]

results = []
for size in dataset_sizes:
    # Generate larger dataset for benchmarking
    X_large = np.random.randn(size, n_features)
    
    fit_time, transform_time = benchmark_pca(X_large, n_components=5)
    
    results.append({
        'dataset_size': size,
        'n_features': n_features,
        'fit_time': fit_time,
        'transform_time': transform_time
    })
    
    print(f"Dataset: {size} samples Ã— {n_features} features")
    print(f"  Fit time: {fit_time:.4f}s")
    print(f"  Transform time: {transform_time:.4f}s")
    print()

# Convert to DataFrame for easier plotting
results_df = pd.DataFrame(results)

# Plot performance results
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.plot(results_df['dataset_size'], results_df['fit_time'], 'bo-', label='Fit Time')
plt.xlabel('Dataset Size')
plt.ylabel('Time (seconds)')
plt.title('PCA Fit Time vs Dataset Size')
plt.grid(True, alpha=0.3)
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(results_df['dataset_size'], results_df['transform_time'], 'ro-', label='Transform Time')
plt.xlabel('Dataset Size')
plt.ylabel('Time (seconds)')
plt.title('PCA Transform Time vs Dataset Size')
plt.grid(True, alpha=0.3)
plt.legend()

plt.tight_layout()
plt.show()
# Comparison with scikit-learn's PCA (for validation)
from sklearn.decomposition import PCA as SKPCA

# Use scikit-learn PCA for comparison
pca_sklearn = SKPCA(n_components=2)
X_pca_sklearn = pca_sklearn.fit_transform(X_scaled)

print("Comparison with scikit-learn PCA:")
print("=" * 40)
print(f"Custom PCA explained variance: {pca_custom.explained_variance_ratio_[:2]}")
print(f"Sklearn PCA explained variance: {pca_sklearn.explained_variance_ratio_}")

# Check if results are similar (allowing for sign flips which are common in PCA)
correlation_pc1 = np.corrcoef(X_pca_custom[:, 0], X_pca_sklearn[:, 0])[0, 1]
correlation_pc2 = np.corrcoef(X_pca_custom[:, 1], X_pca_sklearn[:, 1])[0, 1]

print(f"\nCorrelation between custom and sklearn PC1: {correlation_pc1:.6f}")
print(f"Correlation between custom and sklearn PC2: {correlation_pc2:.6f}")

# Visualization comparison
plt.figure(figsize=(15, 6))

plt.subplot(1, 2, 1)
plt.scatter(X_pca_custom[:, 0], X_pca_custom[:, 1], alpha=0.7)
plt.xlabel('PC1 (Custom)')
plt.ylabel('PC2 (Custom)')
plt.title('Custom PCA Implementation')

plt.subplot(1, 2, 2)
plt.scatter(X_pca_sklearn[:, 0], X_pca_sklearn[:, 1], alpha=0.7)
plt.xlabel('PC1 (Sklearn)')
plt.ylabel('PC2 (Sklearn)')
plt.title('Scikit-learn PCA')

plt.tight_layout()
plt.show()
# Final insights and conclusions
print("PCA ANALYSIS SUMMARY")
print("=" * 50)
print(f"Original dataset dimensions: {df_clean.shape}")
print(f"Data reduction with 2 components: {2/df_clean.shape[1]*100:.1f}% of original features")
print(f"Variance retained with 2 components: {pca_custom.get_cumulative_variance()[1]*100:.1f}%")
print(f"Optimal components for 95% variance: {pca_95.n_components_}")

print("\nKEY INSIGHTS:")
print("1. The first principal component captures the majority of variance in African development data")
print("2. Countries can be effectively clustered in 2D PCA space while retaining most information")
print("3. Economic indicators (GDP, investment) heavily influence the first component")
print("4. Social indicators (life expectancy, literacy) influence the second component")
print("5. PCA successfully reduces dimensionality while preserving data structure")
